# -*- coding: utf-8 -*-
"""Parkinson.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1z7XGQz9-mmR7AkuaK6AGKpMt7t7VuS3D
"""

from google.colab import drive
drive.flush_and_unmount()

from google.colab import drive
drive.mount('/content/drive')

"""##Exploratory data analysis (User)










"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

sns.set_style("whitegrid")
plt.rcParams["figure.figsize"] = (12, 6)

BASE_PATH = "/content/drive/MyDrive/Colab Notebooks/pd_project/data/processed/"

USER_DATA_PATH = BASE_PATH + "cleaned_user_data.csv"
KEYSTROKES_PATH = BASE_PATH + "final_keystrokes_clean.csv"

# Load user metadata
users_df = pd.read_csv(USER_DATA_PATH)

# Read the keystrokes CSV file without specifying usecols
temp_df = pd.read_csv(KEYSTROKES_PATH, nrows=0) # Read only the header
print("Columns in final_keystrokes_clean.csv:", temp_df.columns.tolist())

users_df = pd.read_csv(USER_DATA_PATH)

print("Users dataset shape:", users_df.shape)

keystrokes_df = pd.read_csv(
      KEYSTROKES_PATH,
          usecols=["SourceFile", "UserKey", "Hand (L or R)", "Hold time", "Latency time", "Flight time"],
          dtype={
               "SourceFile": "category",
               "UserKey": "category",
               "Hand (L or R)": "category",
           }
)

# Convert time columns to numeric, coercing errors
keystrokes_df['Hold time'] = pd.to_numeric(keystrokes_df['Hold time'], errors='coerce')
keystrokes_df['Latency time'] = pd.to_numeric(keystrokes_df['Latency time'], errors='coerce')
keystrokes_df['Flight time'] = pd.to_numeric(keystrokes_df['Flight time'], errors='coerce')

print("Keystrokes dataset shape:", keystrokes_df.shape)

print("User Data Sample:")
display(users_df.head())

print("Keystroke Data Sample:")
display(keystrokes_df.head())

print("\nUsers Info:")
users_df.info()

print("\nKeystrokes Info:")
keystrokes_df.info()

print("Missing values in Users dataset:\n", users_df.isnull().sum())
print("\nMissing values in Keystrokes dataset:\n", keystrokes_df.isnull().sum())

sns.countplot(x='Parkinsons', data=users_df, palette='coolwarm')
plt.title("Parkinson's Diagnosis Distribution")
plt.xlabel("Parkinson's Status")
plt.ylabel("Number of Users")
plt.show()

print(users_df['Parkinsons'].value_counts())

sns.histplot(users_df['AgeAtDiagnosis'], bins=20, kde=True, color='green')
plt.title("Age at Diagnosis Distribution")
plt.xlabel("Age at Diagnosis")
plt.ylabel("Frequency")
plt.show()

sns.countplot(x='Impact', data=users_df,
              order=['No Impact', 'Mild', 'Medium', 'Severe'],
              palette='viridis')
plt.title("Parkinson's Severity Levels")
plt.xlabel("Severity Level")
plt.ylabel("Number of Users")
plt.show()

print(users_df['Impact'].value_counts())

"""##Exploratory data analysis (KeyStrokes)"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns


users_df = pd.read_csv(USER_DATA_PATH)

keystrokes_df = pd.read_csv(KEYSTROKES_PATH, usecols=["SourceFile", "UserKey", "Hand (L or R)", "Hold time", "Latency time", "Flight time"])




keystrokes_df['UserID'] = keystrokes_df['SourceFile'].apply(lambda x: x.split('_')[0])

merged_df = pd.merge(keystrokes_df, users_df[['UserID','Parkinsons']], on='UserID', how='left')


merged_df['Hold time'] = pd.to_numeric(merged_df['Hold time'], errors='coerce')


merged_df.dropna(subset=['Hold time'], inplace=True)


# Distribution of HoldTime
plt.figure(figsize=(8,5))
sns.histplot(merged_df['Hold time'], bins=50, kde=True)
plt.title("Distribution of Hold Time")
plt.xlabel("Hold Time (ms)")
plt.ylabel("Frequency")
plt.show()

# Compare HoldTime by Parkinson's
plt.figure(figsize=(8,5))
sns.violinplot(data=merged_df, x='Parkinsons', y='Hold time')
plt.title("Hold Time Comparison: Parkinsonâ€™s vs Control")
plt.xlabel("Parkinson's Status")
plt.ylabel("Hold Time (ms)")
plt.show()

plt.figure(figsize=(8,5))

# Ensure numeric
keystrokes_df['Flight time'] = pd.to_numeric(keystrokes_df['Flight time'], errors='coerce')


sns.histplot(keystrokes_df['Flight time'], bins=50, kde=True)

plt.xlim(0, 25000)
plt.title("Distribution of Flight Time")
plt.xlabel("Flight time")
plt.ylabel("Count")
plt.show()

# Ensure time columns are numeric before aggregation
keystrokes_df['Hold time'] = pd.to_numeric(keystrokes_df['Hold time'], errors='coerce')
keystrokes_df['Flight time'] = pd.to_numeric(keystrokes_df['Flight time'], errors='coerce')
keystrokes_df['Latency time'] = pd.to_numeric(keystrokes_df['Latency time'], errors='coerce')


keystroke_features = keystrokes_df.groupby('UserKey').agg({
    'Hold time': ['mean','std','median'],
    'Flight time': ['mean','std','median'],
    'Latency time': ['mean','std','median']
}).reset_index()

# Flatten columns
keystroke_features.columns = ['UserKey'] + ['_'.join(col) for col in keystroke_features.columns[1:]]

final_df = users_df.merge(
      keystroke_features,
      left_on="UserID",   # check: in your user dataset it might be UserID
      right_on="UserKey",
      how="inner"
 )

print("Final dataset shape:", final_df.shape)
final_df.head()

import os

# Save final merged dataset
output_path = "/content/drive/MyDrive/Colab Notebooks/pd_project/data/processed/combined_dataset.csv"

# Create the directory if it doesn't exist
output_dir = os.path.dirname(output_path)
if not os.path.exists(output_dir):
    os.makedirs(output_dir)

final_df.to_csv(output_path, index=False)

print(f"Final dataset saved at: {output_path}")
print("Shape:", final_df.shape)

plt.figure(figsize=(8,5))
sns.violinplot(data=final_df, x='Parkinsons', y='Hold time_mean')
plt.title("Average Hold Time: Parkinsonâ€™s vs Control")
plt.show()

"""##ML Models"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier

df = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/pd_project/data/processed/combined_dataset.csv")

df.head()

df['Parkinsons'] = df['Parkinsons'].astype(str).str.strip().str.lower().map({
      'true': 1, 'false': 0, 'yes': 1, 'no': 0, '1': 1, '0': 0
 })
 # Drop rows with missing target
df = df.dropna(subset=['Parkinsons'])
df['Parkinsons'] = df['Parkinsons'].astype(int)

# Do the same for Tremors
df['Tremors'] = df['Tremors'].astype(str).str.strip().str.lower().map({
          'true': 1, 'false': 0, 'yes': 1, 'no': 0, '1': 1, '0': 0
})

df.head()

# Encode UPDRS if it's not numeric
if df['UPDRS'].dtype == 'object':
    le = LabelEncoder()
    df['UPDRS'] = le.fit_transform(df['UPDRS'].astype(str))

# One-hot encode Gender, Impact, Sided
df = pd.get_dummies(df, columns=['Gender','Impact','Sided'], drop_first=True)

# Select only the biologically relevant + keystroke features
selected_features = [
    'AgeAtDiagnosis', 'UPDRS',
    'Hold time_mean','Hold time_std','Hold time_median',
    'Flight time_mean','Flight time_std','Flight time_median',
    'Latency time_mean','Latency time_std','Latency time_median'
]

X = df[selected_features]
y = df['Parkinsons']

                # Handle missing values
from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy='median')
X = pd.DataFrame(imputer.fit_transform(X), columns=selected_features)

X_train, X_test, y_train, y_test = train_test_split(
      X, y, test_size=0.2, random_state=42, stratify=y
)

from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier

models = {
      "Logistic Regression": LogisticRegression(max_iter=1000, random_state=42),
      "SVM": SVC(probability=True, random_state=42),
      "Random Forest": RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42),

     # Bagging models
     "Bagging (Decision Tree)": BaggingClassifier(
                                  estimator=DecisionTreeClassifier(max_depth=5, random_state=42),
                                  n_estimators=100, max_samples=0.8, max_features=0.8, random_state=42
      ),
     "Bagging (LogReg)": BaggingClassifier(
                            estimator=LogisticRegression(max_iter=1000, random_state=42),
                            n_estimators=50, max_samples=0.8, max_features=1.0, random_state=42
      ),
     "Bagging (SVM)": BaggingClassifier(
                            estimator=SVC(probability=True, random_state=42),
                            n_estimators=30, max_samples=0.8, max_features=1.0, random_state=42
     )
}

results = {}

for name, model in models.items():
    print("="*60)
    print(f"Training {name}...")
    model.fit(X_train, y_train)

    y_pred = model.predict(X_test)
    y_proba = model.predict_proba(X_test)[:,1] if hasattr(model, "predict_proba") else None

    results[name] = {
         "Accuracy": accuracy_score(y_test, y_pred),
         "Precision": precision_score(y_test, y_pred),
         "Recall": recall_score(y_test, y_pred),
         "F1-score": f1_score(y_test, y_pred),
         "ROC-AUC": roc_auc_score(y_test, y_proba) if y_proba is not None else None
     }

import matplotlib.pyplot as plt

importances = rf.fit(X_train, y_train).feature_importances_
feat_importances = pd.Series(importances, index=X.columns).sort_values(ascending=False)

feat_importances.head(10).plot(kind='barh')
plt.title("Top Features (Random Forest)")
plt.show()

results_df = pd.DataFrame(results)
print("\nSummary of Baseline Models:")
display(results_df)

"""##GRU"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.utils import class_weight
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, roc_auc_score
import matplotlib.pyplot as plt
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import GRU, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras import regularizers

X_sequences = np.load("/content/drive/MyDrive/Colab Notebooks/pd_project/data/X_sequences.npy")
y_labels    = np.load("/content/drive/MyDrive/Colab Notebooks/pd_project/data/y_labels.npy")
user_keys   = np.load("/content/drive/MyDrive/Colab Notebooks/pd_project/data/user_keys.npy")

print("Loaded shapes:", X_sequences.shape, y_labels.shape, user_keys.shape)

USER_DATA_PATH = "/content/drive/MyDrive/Colab Notebooks/pd_project/data/processed/combined_dataset.csv"
users_df = pd.read_csv(USER_DATA_PATH, usecols=["UserKey","Parkinsons"])
unique_users = users_df.drop_duplicates("UserKey")

unique_users.info()

train_users, temp_users = train_test_split(
    unique_users, test_size=0.3, stratify=unique_users["Parkinsons"], random_state=42
)
val_users, test_users = train_test_split(
    temp_users, test_size=0.5, stratify=temp_users["Parkinsons"], random_state=42
)

def get_indices(userkeys):
    return [i for i, uid in enumerate(user_keys) if uid in set(userkeys)]

train_idx = get_indices(train_users["UserKey"])
val_idx   = get_indices(val_users["UserKey"])
test_idx  = get_indices(test_users["UserKey"])

X_train, y_train = X_sequences[train_idx], y_labels[train_idx]
X_val, y_val     = X_sequences[val_idx],   y_labels[val_idx]
X_test, y_test   = X_sequences[test_idx],  y_labels[test_idx]

y_train_cat = to_categorical(y_train, num_classes=2)
y_val_cat   = to_categorical(y_val,   num_classes=2)
y_test_cat  = to_categorical(y_test,  num_classes=2)

print("Train:", X_train.shape, "Val:", X_val.shape, "Test:", X_test.shape)

cw = class_weight.compute_class_weight(
      "balanced", classes=np.unique(y_train), y=y_train
      )
class_weight_dict = dict(enumerate(cw))
print("Class Weights:", class_weight_dict)

SEQ_LEN = X_sequences.shape[1]
NUM_FEATS = X_sequences.shape[2]

gru_model = Sequential([
    GRU(32, return_sequences=True, input_shape=(SEQ_LEN, NUM_FEATS), kernel_regularizer=regularizers.l2(1e-4)),
    Dropout(0.4),
    GRU(16, kernel_regularizer=regularizers.l2(1e-4)),
    Dropout(0.4),
    Dense(16, activation="relu", kernel_regularizer=regularizers.l2(1e-4)),
    Dense(2, activation="softmax")
])

gru_model.compile(optimizer="adam", loss="categorical_crossentropy", metrics=["accuracy"])
gru_model.summary()

early = EarlyStopping(monitor="val_loss", patience=10, restore_best_weights=True)

reduce_lr = ReduceLROnPlateau(monitor="val_loss", factor=0.5, patience=3)

history = gru_model.fit(
    X_train, y_train_cat,
    validation_data=(X_val, y_val_cat),
    epochs=50,
    batch_size=32,
    class_weight=class_weight_dict,
    callbacks=[early, reduce_lr],
    verbose=1
)

val_loss, val_acc = gru_model.evaluate(X_val, y_val_cat)
print(f"Validation Accuracy (per window): {val_acc:.2f}")

# Plot learning curve
plt.plot(history.history['accuracy'], label="Train Acc")
plt.plot(history.history['val_accuracy'], label="Val Acc")
plt.title("GRU Accuracy")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend()
plt.show()

# Get predictions for validation set
y_val_proba = gru_model.predict(X_val)
val_results = pd.DataFrame({
    "UserKey": user_keys[val_idx],
    "TrueLabel": y_val,
    "PredProb": y_val_proba[:,1]
})

# Aggregate per user
user_preds = val_results.groupby("UserKey").agg({
    "TrueLabel": "first",
    "PredProb": "mean"
})
user_preds["PredLabel"] = (user_preds["PredProb"] >= 0.5).astype(int)

print("User-level Accuracy:", accuracy_score(user_preds["TrueLabel"], user_preds["PredLabel"]))
print("User-level ROC-AUC:", roc_auc_score(user_preds["TrueLabel"], user_preds["PredProb"]))

"""## Hybrid LSTM"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.utils import class_weight
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, roc_auc_score
import matplotlib.pyplot as plt
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

X_sequences = np.load("/content/drive/MyDrive/Colab Notebooks/pd_project/data/X_sequences.npy")
y_labels    = np.load("/content/drive/MyDrive/Colab Notebooks/pd_project/data/y_labels.npy")
user_keys   = np.load("/content/drive/MyDrive/Colab Notebooks/pd_project/data/user_keys.npy")

print("Loaded shapes:", X_sequences.shape, y_labels.shape, user_keys.shape)

USER_DATA_PATH = "/content/drive/MyDrive/Colab Notebooks/pd_project/data/processed/combined_dataset.csv"
users_df = pd.read_csv(USER_DATA_PATH, usecols=["UserKey","Parkinsons"])
unique_users = users_df.drop_duplicates("UserKey")
unique_users.info()

train_users, temp_users = train_test_split(
    unique_users, test_size=0.3, stratify=unique_users["Parkinsons"], random_state=42
)
val_users, test_users = train_test_split(
    temp_users, test_size=0.5, stratify=temp_users["Parkinsons"], random_state=42
)

def get_indices(userkeys):
    return [i for i, uid in enumerate(user_keys) if uid in set(userkeys)]

train_idx = get_indices(train_users["UserKey"])
val_idx   = get_indices(val_users["UserKey"])
test_idx  = get_indices(test_users["UserKey"])

X_train, y_train = X_sequences[train_idx], y_labels[train_idx]
X_val, y_val     = X_sequences[val_idx],   y_labels[val_idx]
X_test, y_test   = X_sequences[test_idx],  y_labels[test_idx]

y_train_cat = to_categorical(y_train, num_classes=2)
y_val_cat   = to_categorical(y_val,   num_classes=2)
y_test_cat  = to_categorical(y_test,  num_classes=2)

print("Train:", X_train.shape, "Val:", X_val.shape, "Test:", X_test.shape)

cw = class_weight.compute_class_weight(
      "balanced", classes=np.unique(y_train), y=y_train
      )
class_weight_dict = dict(enumerate(cw))
print("Class Weights:", class_weight_dict)

SEQ_LEN = X_sequences.shape[1]
NUM_FEATS = X_sequences.shape[2]

cnn_lstm_model = Sequential([
    Conv1D(filters=64, kernel_size=5, activation="relu", input_shape=(SEQ_LEN, NUM_FEATS)),
    MaxPooling1D(pool_size=2),
    Dropout(0.3),

    Conv1D(filters=128, kernel_size=5, activation="relu"),
    MaxPooling1D(pool_size=2),
    Dropout(0.3),

    LSTM(64),
    Dropout(0.4),

    Dense(64, activation="relu"),
    Dropout(0.3),
    Dense(2, activation="softmax")
])

cnn_lstm_model.compile(optimizer="adam", loss="categorical_crossentropy", metrics=["accuracy"])
cnn_lstm_model.summary()

early = EarlyStopping(monitor="val_loss", patience=10, restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(monitor="val_loss", factor=0.5, patience=3)

history = cnn_lstm_model.fit(
    X_train, y_train_cat,
    validation_data=(X_val, y_val_cat),
    epochs=25,
    batch_size=32,
    class_weight=class_weight_dict,
    callbacks=[early, reduce_lr],
    verbose=1
)

val_loss, val_acc = cnn_lstm_model.evaluate(X_val, y_val_cat)
print(f"Validation Accuracy (per window): {val_acc:.2f}")

plt.plot(history.history['accuracy'], label="Train Acc")
plt.plot(history.history['val_accuracy'], label="Val Acc")
plt.title("CNN+LSTM Accuracy")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend()
plt.show()

# Save
cnn_lstm_model.save("/content/drive/MyDrive/Colab Notebooks/pd_project/models/cnn_lstm_model.h5")
print("Model saved!")

# Load later
from tensorflow.keras.models import load_model
model2 = load_model("/content/drive/MyDrive/Colab Notebooks/pd_project/models/cnn_lstm_model.h5")

y_test_proba = model2.predict(X_test)
y_test_pred = np.argmax(y_test_proba, axis=1)

print("Per-window Test Accuracy:", accuracy_score(y_test, y_test_pred))
print(classification_report(y_test, y_test_pred))

test_results = pd.DataFrame({
      "UserKey": user_keys[test_idx],
          "TrueLabel": y_test,
              "PredProb": y_test_proba[:,1]
              })

# Group by user â†’ take mean probability
user_preds = test_results.groupby("UserKey").agg({
    "TrueLabel": "first",
    "PredProb": "mean"
})
user_preds["PredLabel"] = (user_preds["PredProb"] >= 0.5).astype(int)

print("User-level Accuracy:", accuracy_score(user_preds["TrueLabel"], user_preds["PredLabel"]))
print("User-level ROC-AUC:", roc_auc_score(user_preds["TrueLabel"], user_preds["PredProb"]))
print("User-level Report:\n", classification_report(user_preds["TrueLabel"], user_preds["PredLabel"]))
print("Confusion Matrix:\n", confusion_matrix(user_preds["TrueLabel"], user_preds["PredLabel"]))

"""##InceptionTime"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras import layers, models
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.utils import class_weight
import matplotlib.pyplot as plt

X = np.load("/content/drive/MyDrive/Colab Notebooks/pd_project/data/X_sequences.npy")
y = np.load("/content/drive/MyDrive/Colab Notebooks/pd_project/data/y_labels.npy")

print("X shape:", X.shape)
print("y shape:", y.shape)

n_samples, seq_len, n_features = X.shape
X_flat = X.reshape(-1, n_features)

scaler = StandardScaler().fit(X_flat)
X_scaled = scaler.transform(X_flat).reshape(n_samples, seq_len, n_features)

# One-hot encode labels if not already
y = tf.keras.utils.to_categorical(y, num_classes=2)

X_train_temp, X_test, y_train_temp, y_test = train_test_split(
      X_scaled, y, test_size=0.2, random_state=42, stratify=y
      )

# Split the temporary training set into actual training and validation sets
X_train, X_val, y_train, y_val = train_test_split(
      X_train_temp, y_train_temp, test_size=0.125, random_state=42, stratify=y_train_temp
)

print("Train:", X_train.shape)
print("Val:", X_val.shape)
print("Test:", X_test.shape)

cw = class_weight.compute_class_weight(
      "balanced", classes=np.unique(np.argmax(y_train, axis=1)), y=np.argmax(y_train, axis=1)
      )
class_weights = dict(enumerate(cw))
print("Class Weights:", class_weights)

def inception_module(input_tensor, stride=1, nb_filters=32, kernel_sizes=[10, 20, 40]):
    conv_list = []
    for k in kernel_sizes:
        conv = layers.Conv1D(filters=nb_filters, kernel_size=k, strides=stride,
                                       padding="same", activation="relu",
                                       kernel_regularizer=tf.keras.regularizers.l2(1e-4))(input_tensor)
        conv_list.append(conv)
    maxpool = layers.MaxPooling1D(pool_size=3, strides=stride, padding="same")(input_tensor)
    conv_maxpool = layers.Conv1D(filters=nb_filters, kernel_size=1,
                                   padding="same", activation="relu")(maxpool)
    conv_list.append(conv_maxpool)
    x = layers.Concatenate(axis=2)(conv_list)
    x = layers.BatchNormalization()(x)
    x = layers.Activation("relu")(x)
    return x

def residual_block(input_tensor, nb_filters=32):
      x = inception_module(input_tensor, nb_filters=nb_filters)
      x = inception_module(x, nb_filters=nb_filters)
      x = inception_module(x, nb_filters=nb_filters)
      shortcut = layers.Conv1D(nb_filters*4, kernel_size=1, padding="same")(input_tensor)
      x = layers.Add()([x, shortcut])
      x = layers.BatchNormalization()(x)
      x = layers.Activation("relu")(x)
      return x

def build_inception_time(input_shape, n_classes=2, nb_filters=32, depth=3):
    input_layer = layers.Input(shape=input_shape)
    x = input_layer
    for _ in range(depth):
        x = residual_block(x, nb_filters=nb_filters)
    gap = layers.GlobalAveragePooling1D()(x)
    output_layer = layers.Dense(n_classes, activation="softmax")(gap)
    model = models.Model(inputs=input_layer, outputs=output_layer)
    return model

inception_time_model = build_inception_time(input_shape=(seq_len, n_features), n_classes=2)

inception_time_model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),
    loss="categorical_crossentropy",
    metrics=["accuracy"]
)

inception_time_model.summary()

callbacks = [
      tf.keras.callbacks.EarlyStopping(
              monitor='val_loss', patience=15, restore_best_weights=True
                  ),
      tf.keras.callbacks.ReduceLROnPlateau(
              monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6
      )
     ]

history = inception_time_model.fit(
      X_train, y_train,
      validation_data=(X_val, y_val),

      epochs=100,
      batch_size=64,
      class_weight=class_weights,
      callbacks=callbacks,
      verbose=1
)

best_epoch = np.argmax(history.history['val_accuracy'])
best_val_acc = history.history['val_accuracy'][best_epoch]
print(f"Best validation accuracy: {best_val_acc:.4f} at epoch {best_epoch+1}")

test_loss, test_acc = inception_time_model.evaluate(X_test, y_test, verbose=0)
print(f"Test Accuracy: {test_acc:.4f}")
plt.figure(figsize=(10,4))
plt.plot(history.history['accuracy'], label='Train Acc')
plt.plot(history.history['val_accuracy'], label='Val Acc')
plt.title("Training History - InceptionTime")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()
plt.show()

inception_time_model.save("/content/drive/MyDrive/Colab Notebooks/pd_project/models/inception_time_model.h5")
print("model saved! ")

"""##Inception time with GAN Data








"""

import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Load the balanced dataset
X = np.load("/content/drive/MyDrive/Colab Notebooks/pd_project/data/processed/X_aug.npy")
y = np.load("/content/drive/MyDrive/Colab Notebooks/pd_project/data/processed/y_aug.npy")

print("Loaded data shapes:", X.shape, y.shape)

from sklearn.model_selection import train_test_split

X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)

print("Train:", X_train.shape, "Val:", X_val.shape, "Test:", X_test.shape)

import tensorflow as tf
from tensorflow.keras import layers, models, regularizers

def inception_module(input_tensor, nb_filters=32, kernel_sizes=[10, 20, 40], bottleneck_size=32):
    # Bottleneck layer (1x1 convolution)
    if bottleneck_size > 0:
        x = layers.Conv1D(bottleneck_size, 1, padding='same', activation='relu')(input_tensor)
    else:
        x = input_tensor

    # Parallel convolutions
    conv_list = []
    for ks in kernel_sizes:
        conv_list.append(layers.Conv1D(nb_filters, ks, padding='same', activation='relu')(x))

    # Max pooling branch
    pool = layers.MaxPooling1D(pool_size=3, strides=1, padding='same')(input_tensor)
    pool_conv = layers.Conv1D(nb_filters, 1, padding='same', activation='relu')(pool)

    # Concatenate all branches
    x = layers.Concatenate(axis=2)(conv_list + [pool_conv])
    x = layers.BatchNormalization()(x)
    x = layers.Activation('relu')(x)
    return x

def build_inception_time(input_shape, nb_classes=1):
    input_layer = layers.Input(shape=input_shape)

    # Stack of Inception modules
    x = inception_module(input_layer, nb_filters=32)
    x = inception_module(x, nb_filters=32)
    x = inception_module(x, nb_filters=32)

    # Global Average Pooling
    gap = layers.GlobalAveragePooling1D()(x)

    # Output layer
    output_layer = layers.Dense(nb_classes, activation='sigmoid')(gap)

    model = models.Model(inputs=input_layer, outputs=output_layer)

    model.compile(
        loss='binary_crossentropy',
        optimizer=tf.keras.optimizers.Adam(1e-3),
        metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]
    )
    return model

input_shape = (X_train.shape[1], X_train.shape[2])
model = build_inception_time(input_shape)

callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

history = model.fit(
    X_train, y_train,
    epochs=50,
    batch_size=64,
    validation_data=(X_val, y_val),
    callbacks=[callback],
    verbose=1
)

import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns

# Evaluate
test_loss, test_acc, test_auc = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {test_acc:.4f} | AUC: {test_auc:.4f}")

# Predictions
y_pred = (model.predict(X_test) > 0.5).astype(int)

# Classification Report
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Purples', xticklabels=['Control', 'Parkinson'], yticklabels=['Control', 'Parkinson'])
plt.title("InceptionTime Confusion Matrix")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.show()

model_save_path = "/content/drive/MyDrive/Colab Notebooks/pd_project/models/inception_time_v2_model.keras"

model.save(model_save_path)

print(f"InceptionTime model saved successfully at:\n{model_save_path}")

from tensorflow.keras.models import load_model

# Load your saved InceptionTime model
model_path = "/content/drive/MyDrive/Colab Notebooks/pd_project/models/inception_time_v2_model.keras"
model = load_model(model_path)

print("Model loaded successfully!")

# Example using one real test sequence
sample_input = X_test[0:1]  # Shape: (1, 500, 3)
true_label = y_test[0]

# Predict
prediction = model.predict(sample_input)
probability = float(prediction[0][0])
predicted_label = "Parkinson's" if probability >= 0.5 else "Control"

print(f" True Label: {'Parkinson' if true_label==1 else 'Control'}")
print(f" Predicted Label: {predicted_label}")
print(f" Parkinsonâ€™s Probability: {probability:.4f}")

predictions = (model.predict(X_test[:10]) > 0.5).astype(int)

for i, p in enumerate(predictions):
    print(f"Sample {i+1}: Predicted -> {'Parkinson' if p==1 else 'Control'}, True -> {'Parkinson' if y_test[i]==1 else 'Control'}")

preds = model.predict(X_test)
print(np.mean(preds[y_test == 0]), np.mean(preds[y_test == 1]))

import numpy as np

# assuming model expects (None, 500, 3)
dummy_input = np.random.rand(1, 500, 3).astype("float32")
prediction = model.predict(dummy_input)

print("Prediction:", prediction)

import numpy as np
random_input = np.random.normal(size=(1, 500, 3))
print(model.predict(random_input))

import numpy as np
from sklearn.preprocessing import StandardScaler
import joblib

# Load your augmented training sequences
X = np.load("/content/drive/MyDrive/Colab Notebooks/pd_project/data/processed/X_aug.npy")

print("Loaded data shape:", X.shape)  # should be (samples, 500, 3)
SEQ_LEN = X.shape[1]
NUM_FEATS = X.shape[2]

# Fit a new scaler on flattened data
scaler = StandardScaler()
scaler.fit(X.reshape(-1, NUM_FEATS))

# Save the scaler for inference and backend use
joblib.dump(scaler, "/content/drive/MyDrive/Colab Notebooks/pd_project/models/scaler.pkl")
print(" Scaler saved successfully at /content/drive/MyDrive/Colab Notebooks/pd_project/models/scaler.pkl")

from tensorflow.keras.models import load_model

# Load your model
model = load_model("/content/drive/MyDrive/Colab Notebooks/pd_project/models/inception_time_v2_model.keras")

# Load the scaler you just saved
scaler = joblib.load("/content/drive/MyDrive/Colab Notebooks/pd_project/models/scaler.pkl")

# Create a realistic test input (like 500 keystrokes with 3 timing features)
dummy_keystrokes = np.random.uniform(low=50, high=200, size=(500, 3))

# Apply same scaling
scaled_input = scaler.transform(dummy_keystrokes).reshape(1, 500, 3)

# Predict
prediction = model.predict(scaled_input)
prob = float(prediction[0][0])
label = "Parkinsonâ€™s Detected" if prob >= 0.5 else "Healthy"

print(f"ðŸ§  Prediction: {label} (prob={prob:.4f})")

print("Mean:", scaler.mean_)
print("Std Dev:", scaler.scale_)

# ==============================================================
#  1. Imports
# ==============================================================
import numpy as np
import pandas as pd
import joblib
from tensorflow.keras.models import load_model

# ==============================================================
#  2. Paths (update these for your setup)
# ==============================================================
CSV_PATH = "/content/drive/MyDrive/Colab Notebooks/pd_project/data/healthy_user.csv"  # your CSV path
MODEL_PATH = "/content/drive/MyDrive/Colab Notebooks/pd_project/models/inception_time_v2_model.keras"
SCALER_PATH = "/content/drive/MyDrive/Colab Notebooks/pd_project/models/scaler.pkl"

SEQ_LEN = 500
FEATURES = ["holdTime", "flightTime", "latencyTime"]

# ==============================================================
#  3. Load model and scaler
# ==============================================================
print("ðŸ“¦ Loading model and scaler ...")
model = load_model(MODEL_PATH, compile=True)
scaler = joblib.load(SCALER_PATH)
print("âœ… Model and scaler loaded successfully.")

# ==============================================================
#  4. Load and clean CSV keystroke data
# ==============================================================
df = pd.read_csv(CSV_PATH, on_bad_lines='skip')
print(f"Original shape: {df.shape}")
print("Columns:", df.columns.tolist())

# Select only necessary features
df = df[FEATURES].copy()

# Drop rows with NaN or invalid values
df = df.dropna().apply(pd.to_numeric, errors="coerce").dropna()
print(f"After cleaning: {df.shape}")

# ==============================================================
#  5. Auto-detect ms vs sec units
# ==============================================================
mean_val = df.mean().mean()
if mean_val > 5:  # likely milliseconds
    print("âš  Detected millisecond data â†’ converting to seconds.")
    df[FEATURES] = df[FEATURES] / 1000.0
else:
    print("âœ… Data already in seconds.")

print("Feature means after normalization:\n", df.mean())

# ==============================================================
#  6. Prepare 500-length sequence
# ==============================================================
if len(df) >= SEQ_LEN:
    seq = df.tail(SEQ_LEN).values
else:
    pad_len = SEQ_LEN - len(df)
    pad = np.tile(df.mean().values, (pad_len, 1))
    seq = np.vstack([pad, df.values])

print(f"Final sequence shape: {seq.shape}")

# Scale using the training scaler
seq_scaled = scaler.transform(seq).reshape(1, SEQ_LEN, len(FEATURES))

# ==============================================================
#  7. Predict
# ==============================================================
print("ðŸ§  Running model prediction ...")
prediction = model.predict(seq_scaled)
prob = float(prediction[0][0])

label = "Parkinsonâ€™s Detected" if prob >= 0.5 else "Healthy"
if prob >= 0.85:
    severity = "Severe"
elif prob >= 0.65:
    severity = "Moderate"
elif prob >= 0.5:
    severity = "Mild"
else:
    severity = "None"

# ==============================================================
#  8. Display result
# ==============================================================
print("\n===========================")
print("     ðŸ“ƒ Prediction Report   ")
print("===========================")
print(f"Label: {label}")
print(f"Probability: {prob:.4f}")
print(f"Severity: {severity}")
print("===========================")

import numpy as np

for m in [0.01, 0.05, 0.1, 0.5, 1, 2]:
    t = np.ones((1, 500, 3)) * m
    print(f"Input mean {m} â†’ Pred:", model.predict(t)[0][0])

# ==============================================================
#  5. Normalize using training scaler properly
# ==============================================================

# Convert ms â†’ seconds if avg > 1.0 (heuristic)
if df[FEATURES].mean().mean() > 1.0:
    print("âš  Detected ms data â†’ converting to seconds.")
    df[FEATURES] = df[FEATURES] / 1000.0
else:
    print("âœ… Data already looks like seconds.")

# Align shape to expected sequence length (500)
if len(df) >= SEQ_LEN:
    seq = df.tail(SEQ_LEN).values
else:
    pad_len = SEQ_LEN - len(df)
    pad = np.tile(df.mean().values, (pad_len, 1))
    seq = np.vstack([pad, df.values])

# Scale with the same training scaler
seq_scaled = scaler.transform(seq).reshape(1, SEQ_LEN, len(FEATURES))

# ==============================================================
# 1. Imports
# ==============================================================
import numpy as np
import pandas as pd
import joblib
from tensorflow.keras.models import load_model

# ==============================================================
# 2. Paths (update according to your Drive)
# ==============================================================
CSV_PATH = "/content/drive/MyDrive/Colab Notebooks/pd_project/data/healthy_user.csv"  # your file
MODEL_PATH = "/content/drive/MyDrive/Colab Notebooks/pd_project/models/inception_time_v2_model.keras"
SCALER_PATH = "/content/drive/MyDrive/Colab Notebooks/pd_project/models/scaler.pkl"

SEQ_LEN = 500
FEATURES = ["holdTime", "flightTime", "latencyTime"]

# ==============================================================
# 3. Load model & scaler
# ==============================================================
print("ðŸ“¦ Loading model and scaler ...")
model = load_model(MODEL_PATH, compile=True)
scaler = joblib.load(SCALER_PATH)
print("âœ… Model and scaler loaded successfully.")

# ==============================================================
# 4. Load and inspect keystroke data
# ==============================================================
print("ðŸ“‚ Trying to load CSV file safely...")

def load_csv_with_delimiters(path, delimiters=[',', '\t', ';']):
    for sep in delimiters:
        try:
            df = pd.read_csv(path, sep=sep, on_bad_lines='skip')
            print(f"âœ… Loaded using '{sep}'-separated format.")
            return df
        except pd.errors.ParserError as e:
            print(f"âŒ Failed to load with '{sep}': {e}")
        except Exception as e: # Catch other potential errors like FileNotFoundError
            print(f"âŒ An unexpected error occurred while trying '{sep}': {e}")
    raise Exception("Failed to load CSV with any tried delimiter.")

df = load_csv_with_delimiters(CSV_PATH)

print(f"\nLoaded successfully. Shape: {df.shape}")
print("Columns detected:", df.columns.tolist())

# Keep only required features
df = df[FEATURES].copy()

# Drop NaN and ensure numeric
df = df.dropna().apply(pd.to_numeric, errors="coerce").dropna()
print(f"After cleaning: {df.shape}")

# ==============================================================
# 5. Robust per-feature normalization
# ==============================================================
converted = False
for col in FEATURES:
    mean_val = df[col].mean()
    if mean_val > 1.0:
        print(f"âš  {col} seems to be in milliseconds (mean={mean_val:.2f}) â†’ converting to seconds.")
        df[col] = df[col] / 1000.0
        converted = True

if not converted:
    print("âœ… All features already look like seconds.")

# Cap outliers above 2 seconds (to avoid huge spikes)
df[FEATURES] = df[FEATURES].clip(upper=2.0)

print("Feature means before scaling:\n", df.mean())

# ==============================================================
# 6. Adjust to fixed sequence length (500)
# ==============================================================
if len(df) >= SEQ_LEN:
    seq = df.tail(SEQ_LEN).values
else:
    pad_len = SEQ_LEN - len(df)
    pad = np.tile(df.mean().values, (pad_len, 1))
    seq = np.vstack([pad, df.values])

print(f"Final sequence shape: {seq.shape}")

# ==============================================================
# 7. Scale with training scaler
# ==============================================================
seq_scaled = scaler.transform(seq).reshape(1, SEQ_LEN, len(FEATURES))

print("Scaled feature range:")
print("Min:", seq_scaled.min(), "Max:", seq_scaled.max())

# ==============================================================
# 8. Run prediction
# ==============================================================
print("\nðŸ§  Running model prediction ...")
prediction = model.predict(seq_scaled)
prob = float(prediction[0][0])

label = "Parkinsonâ€™s Detected" if prob >= 0.5 else "Healthy"
if prob >= 0.85:
    severity = "Severe"
elif prob >= 0.65:
    severity = "Moderate"
elif prob >= 0.5:
    severity = "Mild"
else:
    severity = "None"

# ==============================================================
# 9. Display results
# ==============================================================
print("\n===========================")
print("     ðŸ§¾ Prediction Report   ")
print("===========================")
print(f"Label: {label}")
print(f"Probability: {prob:.4f}")
print(f"Severity: {severity}")
print("===========================")

X_existing = np.load("/content/drive/MyDrive/Colab Notebooks/pd_project/data/processed/X_aug.npy")
y_existing = np.load("/content/drive/MyDrive/Colab Notebooks/pd_project/data/processed/y_aug.npy")

# Assuming X and y from the previous cell (New Data Model) are available
# Filter for healthy users (label == 0) from the new data
healthy_indices = np.where(y == 0)[0]
X_healthy = X[healthy_indices]
y_healthy = y[healthy_indices]

print(f"Healthy data from new dataset: X_healthy shape {X_healthy.shape}, y_healthy shape {y_healthy.shape}")

X_combined = np.concatenate([X_existing, X_healthy], axis=0)
y_combined = np.concatenate([y_existing, y_healthy], axis=0)

np.save("X_retrain.npy", X_combined)
np.save("y_retrain.npy", y_combined)

print("Final dataset for retraining (X_retrain, y_retrain):", X_combined.shape, y_combined.shape)

"""## New Data Model

"""

import os
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
import tensorflow as tf
from tensorflow.keras import layers, models, regularizers, callbacks
import joblib, json
import matplotlib.pyplot as plt

BASE_PATH = "/content/drive/MyDrive/Colab Notebooks/pd_project"
DATA_PATH = os.path.join(BASE_PATH, "data/processed")
MODEL_PATH = os.path.join(BASE_PATH, "models")

os.makedirs(MODEL_PATH, exist_ok=True)

INPUT_FILE = os.path.join(DATA_PATH, "combined_keystroke_data_with_userid.csv")
MODEL_FILE = os.path.join(MODEL_PATH, "parkinson_final_gru_model.keras")
SCALER_FILE = os.path.join(MODEL_PATH, "scaler.pkl")
RESULTS_FILE = os.path.join(MODEL_PATH, "results.json")

SEQ_LEN = 400
print("ðŸ“‚ Data path:", DATA_PATH)
print("ðŸ’¾ Models path:", MODEL_PATH)

# Load data
df = pd.read_csv(INPUT_FILE)
df = df[["user_id", "holdTime", "flightTime", "latencyTime", "label"]].dropna().reset_index(drop=True)
print(f"âœ… Loaded {df.shape[0]} rows with {df['user_id'].nunique()} users")

# Normalize
scaler = StandardScaler()
df[["holdTime", "flightTime", "latencyTime"]] = scaler.fit_transform(
    df[["holdTime", "flightTime", "latencyTime"]]
    )
joblib.dump(scaler, SCALER_FILE)
print(f"ðŸ’¾ Scaler saved to {SCALER_FILE}")

X_list, y_list = [], []
for uid, group in df.groupby("user_id"):
    label = group["label"].iloc[0]
    feats = group[["holdTime", "flightTime", "latencyTime"]].values

    # Pad or trim to 400 length
    if len(feats) < SEQ_LEN:
        pad = np.tile(feats[-1], (SEQ_LEN - len(feats), 1))
        feats = np.vstack([feats, pad])
    elif len(feats) > SEQ_LEN:
        feats = feats[:SEQ_LEN]

    X_list.append(feats)
    y_list.append(label)

X = np.array(X_list)
y = np.array(y_list)

print(f"âœ… Sequences built: {X.shape[0]} users Ã— {SEQ_LEN} timesteps Ã— {X.shape[2]} features")

def augment_data(X, y, factor=1):
    X_aug, y_aug = [], []
    for i in range(len(X)):
        for _ in range(factor):
            noise = np.random.normal(0, 0.05, X[i].shape)
            X_noisy = X[i] + noise
            X_aug.append(X_noisy)
            y_aug.append(y[i])
    return np.concatenate([X, np.array(X_aug)]), np.concatenate([y, np.array(y_aug)])

X_aug, y_aug = augment_data(X, y, factor=1)
print("âœ… Augmented data shape:", X_aug.shape)

# 70% train, 15% val, 15% test
X_train, X_temp, y_train, y_temp = train_test_split(X_aug, y_aug, test_size=0.3, stratify=y_aug, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)

print(f"ðŸ“Š Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}")

def build_final_gru(seq_len, n_features):
      model = models.Sequential([
              layers.Input(shape=(seq_len, n_features)),
              layers.Bidirectional(layers.GRU(128, return_sequences=True, kernel_regularizer=regularizers.l2(1e-4))),
              layers.Dropout(0.4),
              layers.Bidirectional(layers.GRU(64, kernel_regularizer=regularizers.l2(1e-4))),
              layers.Dropout(0.4),
              layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l2(1e-4)),
              layers.Dropout(0.3),
              layers.Dense(1, activation='sigmoid')
      ])
      model.compile(
             optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3, clipnorm=1.0),
             loss='binary_crossentropy',
             metrics=['accuracy']
       )
      return model

model = build_final_gru(SEQ_LEN, 3)
model.summary()

es = callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
rlr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)
chkpt = callbacks.ModelCheckpoint(MODEL_FILE, monitor='val_accuracy', save_best_only=True, mode='max')

history = model.fit(
    X_train, y_train,
        validation_data=(X_val, y_val),
            epochs=150,
                batch_size=8,
                    callbacks=[es, rlr, chkpt],
                        verbose=1
                        )

# Validation
val_prob = model.predict(X_val).ravel()
val_pred = (val_prob > 0.5).astype(int)
print("ðŸ“ˆ Validation Results:")
print(classification_report(y_val, val_pred))
print("Confusion Matrix:\n", confusion_matrix(y_val, val_pred))
print("ROC-AUC:", roc_auc_score(y_val, val_prob))

# Test
test_prob = model.predict(X_test).ravel()
test_pred = (test_prob > 0.5).astype(int)
print("\nðŸ§ª Test Results:")
print(classification_report(y_test, test_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, test_pred))
print("ROC-AUC:", roc_auc_score(y_test, test_prob))

plt.figure(figsize=(10,4))
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Val Accuracy')
plt.title('Training Accuracy')
plt.xlabel('Epochs'); plt.ylabel('Accuracy'); plt.legend(); plt.show()

plt.figure(figsize=(10,4))
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.title('Training Loss')
plt.xlabel('Epochs'); plt.ylabel('Loss'); plt.legend(); plt.show()

model.save(MODEL_FILE)
print(f"ðŸ’¾ Model saved at: {MODEL_FILE}")

results = {
    "val_accuracy": float(np.max(history.history['val_accuracy'])),
    "val_loss": float(np.min(history.history['val_loss'])),
    "test_accuracy": float(np.mean(test_pred == y_test)),
    "test_roc_auc": float(roc_auc_score(y_test, test_prob))
}

with open(RESULTS_FILE, "w") as f:
    json.dump(results, f, indent=4)

print(f"ðŸ“Š Metrics saved to: {RESULTS_FILE}")

NEW_FILE = "/content/drive/MyDrive/Colab Notebooks/pd_project/data/healthy_user.csv"

df_new = pd.read_csv(NEW_FILE, on_bad_lines='skip')

# Drop unnamed empty columns automatically
df_new = df_new.loc[:, ~df_new.columns.str.contains('^Unnamed')]

print("âœ… Cleaned columns:", df_new.columns.tolist())

df_new = df_new[["holdTime", "flightTime", "latencyTime"]].dropna().reset_index(drop=True)

df_new_scaled = scaler.transform(df_new)

# Pad/trim to 400
SEQ_LEN = 400
if len(df_new_scaled) < SEQ_LEN:
    pad = np.tile(df_new_scaled[-1], (SEQ_LEN - len(df_new_scaled), 1))
    df_new_scaled = np.vstack([df_new_scaled, pad])
elif len(df_new_scaled) > SEQ_LEN:
    df_new_scaled = df_new_scaled[:SEQ_LEN]

X_new = np.expand_dims(df_new_scaled, axis=0)
print("âœ… Final shape ready for prediction:", X_new.shape)

print(df_new.head(3))
print("Columns:", df_new.columns.tolist())

prob = float(model.predict(X_new)[0][0])

if prob < 0.5:
    status = "Healthy"
elif prob < 0.65:
    status = "Mild Parkinson"
elif prob < 0.85:
    status = "Moderate Parkinson"
else:
    status = "Severe Parkinson"

print(f"ðŸ§  Parkinson Probability: {prob:.4f}")
print(f"ðŸ©º Predicted Status: {status}")

newhealthy = "/content/drive/MyDrive/Colab Notebooks/pd_project/data/healthytest.csv"

df_new = pd.read_csv(newhealthy, on_bad_lines='skip')

# Drop unnamed empty columns automatically
df_new = df_new.loc[:, ~df_new.columns.str.contains('^Unnamed')]

print("âœ… Cleaned columns:", df_new.columns.tolist())

df_new = df_new[["holdTime", "flightTime", "latencyTime"]].dropna().reset_index(drop=True)

df_new_scaled = scaler.transform(df_new)

# Pad/trim to 400
SEQ_LEN = 400
if len(df_new_scaled) < SEQ_LEN:
    pad = np.tile(df_new_scaled[-1], (SEQ_LEN - len(df_new_scaled), 1))
    df_new_scaled = np.vstack([df_new_scaled, pad])
elif len(df_new_scaled) > SEQ_LEN:
    df_new_scaled = df_new_scaled[:SEQ_LEN]

X_new = np.expand_dims(df_new_scaled, axis=0)
print("âœ… Final shape ready for prediction:", X_new.shape)

print(df_new.head(3))
print("Columns:", df_new.columns.tolist())

prob = float(model.predict(X_new)[0][0])

if prob < 0.5:
    status = "Healthy"
elif prob < 0.65:
    status = "Mild Parkinson"
elif prob < 0.85:
    status = "Moderate Parkinson"
else:
    status = "Severe Parkinson"

print(f"ðŸ§  Parkinson Probability: {prob:.4f}")
print(f"ðŸ©º Predicted Status: {status}")

import tensorflow as tf
import joblib
import numpy as np
import pandas as pd

# Paths
BASE_PATH = "/content/drive/MyDrive/Colab Notebooks/pd_project"
MODEL_PATH = f"{BASE_PATH}/models"
MODEL_FILE = f"{MODEL_PATH}/parkinson_final_gru_model.keras"
SCALER_FILE = f"{MODEL_PATH}/scaler.pkl"

# Load model & scaler
model = tf.keras.models.load_model(MODEL_FILE)
scaler = joblib.load(SCALER_FILE)

print("âœ… Model and Scaler loaded successfully!")

= 400  # must match training sequence length
NEW_FILE = "/content/drive/MyDrive/Colab Notebooks/pd_project/data/severepar.csv"  # change this path

# Read CSV (ignore malformed rows)
df = pd.read_csv(NEW_FILE, on_bad_lines='skip')
df = df.loc[:, ~df.columns.str.contains('^Unnamed')]
df = df[["holdTime", "flightTime", "latencyTime"]].dropna().reset_index(drop=True)

# Optional: convert ms â†’ seconds if needed
# df[["holdTime", "flightTime", "latencyTime"]] /= 1000.0

# Normalize
df_scaled = scaler.transform(df)

# Pad / trim to sequence length
if len(df_scaled) < SEQ_LEN:
    pad = np.tile(df_scaled[-1], (SEQ_LEN - len(df_scaled), 1))
    df_scaled = np.vstack([df_scaled, pad])
elif len(df_scaled) > SEQ_LEN:
    df_scaled = df_scaled[:SEQ_LEN]

# Prepare model input
X_new = np.expand_dims(df_scaled, axis=0)SEQ_LEN
print("âœ… Input ready for prediction:", X_new.shape)

# Predict
prob = float(model.predict(X_new, verbose=0)[0][0])

# Interpret
if prob < 0.5:
    status = "Healthy"
elif prob < 0.65:
    status = "Mild Parkinson"
elif prob < 0.85:
    status = "Moderate Parkinson"
else:
    status = "Severe Parkinson"

print(f"ðŸ§  Parkinson Probability: {prob:.4f}")
print(f"ðŸ©º Predicted Status: {status}")

import tensorflow as tf
import joblib
import numpy as np
import pandas as pd

# Paths
BASE_PATH = "/content/drive/MyDrive/Colab Notebooks/pd_project"
MODEL_PATH = f"{BASE_PATH}/models"
MODEL_FILE = f"{MODEL_PATH}/parkinson_final_gru_model.keras"
SCALER_FILE = f"{MODEL_PATH}/scaler.pkl"

# Load model & scaler
model = tf.keras.models.load_model(MODEL_FILE)
scaler = joblib.load(SCALER_FILE)

print("âœ… Model and Scaler loaded successfully!")

SEQ_LEN = 400  # must match training sequence length
NEW_FILE = "/content/drive/MyDrive/Colab Notebooks/pd_project/data/mildpar.csv"  # change this path

# Read CSV (ignore malformed rows)
df = pd.read_csv(NEW_FILE, on_bad_lines='skip')
df = df.loc[:, ~df.columns.str.contains('^Unnamed')]
df = df[["holdTime", "flightTime", "latencyTime"]].dropna().reset_index(drop=True)

# Optional: convert ms â†’ seconds if needed
# df[["holdTime", "flightTime", "latencyTime"]] /= 1000.0

# Normalize
df_scaled = scaler.transform(df)

# Pad / trim to sequence length
if len(df_scaled) < SEQ_LEN:
    pad = np.tile(df_scaled[-1], (SEQ_LEN - len(df_scaled), 1))
    df_scaled = np.vstack([df_scaled, pad])
elif len(df_scaled) > SEQ_LEN:
    df_scaled = df_scaled[:SEQ_LEN]

# Prepare model input
X_new = np.expand_dims(df_scaled, axis=0)
print("âœ… Input ready for prediction:", X_new.shape)

# Predict
prob = float(model.predict(X_new, verbose=0)[0][0])

# Interpret
if prob < 0.5:
    status = "Healthy"
elif prob < 0.65:
    status = "Mild Parkinson"
elif prob < 0.85:
    status = "Moderate Parkinson"
else:
    status = "Severe Parkinson"

print(f"ðŸ§  Parkinson Probability: {prob:.4f}")
print(f"ðŸ©º Predicted Status: {status}")